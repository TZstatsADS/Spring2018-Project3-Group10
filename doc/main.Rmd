---
title: "Project 3"
author: "Group 10"
date: "March 19, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

packages.used=c("gbm", "MASS", "OpenImageR", "jpeg", "ggplot2", "reshape2", "randomForest", 
                "e1071", "xgboost")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

## Loading packages
library("EBImage")
library("gbm")
library("MASS")
library("OpenImageR")
library("jpeg")
library("ggplot2")
library("reshape2")
library("randomForest")
library("e1071")
library("xgboost")
```

### Step 0: specify directories.

Set the working directory to the image folder. Specify the training and the testing set. For data without an independent test/validation set, you need to create your own testing data by random subsampling. In order to obain reproducible results, set.seed() whenever randomization is used. 

```{r wkdir, eval=FALSE}
# setwd("../Spring2018-Project3-Group10") 
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
```

Provide directories for raw images. ~~Training set and test set should be in different subfolders. ~~
```{r}
experiment_dir <- "../data/train/" # This will be modified for different data sets.
# img_train_dir <- paste(experiment_dir, "train/", sep="")
# img_test_dir <- paste(experiment_dir, "test/", sep="")
```

### Step 1: set up controls for evaluation experiments.

In this chunk, ,we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set  
+ (number) K, the number of CV folds  
+ (T/F) process features for images  
+ (T/F) run evaluation on an independent test set 
+ (T/F) process features for test set


```{r exp_setup}
run.cv=F # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature=FALSE # process features
run.test=TRUE # run evaluation on an independent test set
run.test.feature = F # process features for test set
```

Which feature extraction method to perform
```{r}
run.sift = F
run.hog = F
run.lbp = F
```

Which model to run
```{r}
run.gbm = F
run.svm = F
run.xgboost = F
run.rf = F
run.logistic = F
```


Using cross-validation or independent test set evaluation, we compare the performance of different classifiers or classifiers with different specifications. 

```{r model_setup}
model_values <- seq(3, 11, 2) # depth for GBM
model_labels = paste("GBM with depth =", model_values)

svm_values <- seq(0.01, 0.1, by = 0.02) # gamma for svm
svm_labels = paste("SVM with gamma =", svm_values)

xgboost_values <- seq(0.1, 0.3, by = 0.1) # eta for xgboost
xgboost_labels = paste("XGBoost with eta =", xgboost_values)

```


### Step 2: construct visual feature and split

Construct visual feature, then split train and test sets

```{r feature}
# set up feature name
if(run.sift){
   feature_name <- "sift"
}
if(run.hog){
  feature_name <- "hog"
}
if(run.lbp){
  feature_name <- "lbp"
}

# run feature extraction
source("../lib/feature.R")

features <- NULL
if(run.feature){
  features <- feature(experiment_dir, 
                      run.sift = run.sift, run.hog = run.hog, run.lbp = run.lbp,  
                      export = T)
}else{
  if("sift" %in% feature_name){
    load("../output/feature_SIFT.RData")
    features <- sift
  }
  if("hog" %in% feature_name){
    load("../output/feature_HOG.RData")
    features <- hog
  }
  if ("lbp" %in% feature_name){
    features <- read.csv('../output/feature_LBP.csv',header = F)
  }
}

# constructing the feature path name
train_path <- paste0("../output/", feature_name, "_train.csv")
test_path <- paste0("../output/", feature_name, "_test.csv")

# split train and test sets
source("../lib/split_train_test.R")
data <- split_train_test(features)
write.csv(data$train, train_path)
write.csv(data$test, test_path)
#save(data$train, file=paste0("../output/", feature_name, "_train.RData"))
#save(data$test, file=paste0("../output/", feature_name, "_test.RData"))
```

### Step 3: Import train and test sets.

For the example of zip code digits, we code digit 9 as "1" and digit 7 as "0" for binary classification.

```{r train_label}
# read test dataset
dat_test <- read.csv(test_path, header = T)
dat_test <- dat_test[,-1]

# read train dataset
dat_train <- read.csv(train_path, header = T)
dat_train <- dat_train[, 3:length(dat_train[1,])]
label_train <- read.csv(train_path, header = T)
label_train <- label_train[,2]
```

### Step 4: Train a classification model with training images
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
+ Input: a path that points to the training set features.
+ Input: an R object of training sample labels.
+ Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
+ `test.R`
+ Input: a path that points to the test set features.
+ Input: an R object that contains a trained classifier.
+ Output: an R object of class label predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib}
source("../lib/train.R")
source("../lib/test.R")
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth for GBM in this example. 
```{r runcv, message=FALSE, warning=FALSE}
source("../lib/cross_validation.R")

# which model to perform cross validation
cv.gbm = F
cv.svm = F
cv.xgboost = T

if(run.cv){
  if(cv.gbm){
    err_cv <- array(dim=c(length(model_values), 2))
    for(k in 1:length(model_values)){
      cat("k=", k, "\n")
      err_cv[k,] <- cv.function(as.data.frame(dat_train), label_train, model_values[k], K, cv.gbm = T)
    }
  }
  
  if(cv.svm){
    err_cv <- array(dim=c(length(svm_values), 2))
    for(k in 1:length(svm_values)){
      cat("k=", k, "\n")
      err_cv[k,] <- cv.function(as.data.frame(dat_train), label_train, svm_values[k], K, cv.svm = T)
    }
  }
  
  if(cv.xgboost){
    err_cv <- array(dim=c(length(xgboost_values), 2))
    for(k in 1:length(xgboost_values)){
      cat("k=", k, "\n")
      err_cv[k,] <- cv.function(as.data.frame(dat_train), label_train, xgboost_values[k], K, cv.xgboost = T)
    }
  }
  
  save(err_cv, file="../output/err_cv.RData")
}
```

Visualize cross-validation results. 

```{r cv_vis}
if(run.cv){
  if(cv.gbm){
    load("../output/err_cv.RData")
    #pdf("../figs/cv_results.pdf", width=7, height=5)
    plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
         main="Cross Validation Error", type="n", ylim=c(0.25, 0.4))
    points(model_values, err_cv[,1], col="blue", pch=16)
    lines(model_values, err_cv[,1], col="blue")
    arrows(model_values, 
           err_cv[,1]-err_cv[,2], 
           model_values, 
           err_cv[,1]+err_cv[,2], 
           length=0.1, 
           angle=90, 
           code=3)
    #dev.off()
  }
  if(cv.svm){
    load("../output/err_cv.RData")
    #pdf("../figs/cv_results.pdf", width=7, height=5)
    plot(svm_values, err_cv[,1], xlab="SVM gamma values", ylab="CV Error",
         main="Cross Validation Error", type="n", ylim=c(0.30, 0.50))
    points(svm_values, err_cv[,1], col="blue", pch=16)
    lines(svm_values, err_cv[,1], col="blue")
    arrows(svm_values, 
           err_cv[,1]-err_cv[,2], 
           svm_values, 
           err_cv[,1]+err_cv[,2], 
           length=0.1, 
           angle=90, 
           code=3)
    #dev.off()
  }
  if(cv.xgboost){
    load("../output/err_cv.RData")
    #pdf("../figs/cv_results.pdf", width=7, height=5)
    plot(xgboost_values, err_cv[,1], xlab="XGBoost eta values", ylab="CV Error",
         main="Cross Validation Error", type="n", ylim=c(0.30, 0.50))
    points(xgboost_values, err_cv[,1], col="blue", pch=16)
    lines(xgboost_values, err_cv[,1], col="blue")
    arrows(xgboost_values, 
           err_cv[,1]-err_cv[,2], 
           xgboost_values, 
           err_cv[,1]+err_cv[,2], 
           length=0.1, 
           angle=90, 
           code=3)
    #dev.off()
  }
  
}

```


* Choose the "best" parameter value
```{r best_model}
if(run.cv){
  model_best=model_values[1]
  # Best parameter for GBM
  if(run.gbm){
    model_best <- model_values[which.min(err_cv[,1])]
  }
}

par_best <- list(depth=model_best)
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train}

if(run.gbm){
  tm_train <- system.time(model.gbm <- train(as.data.frame(dat_train), label_train, params = model_best))
  save(model.gbm, file="../output/model_gbm.RData")
}


```

### Step 5: Make prediction 
Feed the final training model with the completely holdout testing data. 
```{r test}
# tm_test=NA
if(run.gbm){
  load(file="../output/model_gbm.RData")
  tm_test_gbm <- system.time(pred_gbm <- test(model.gbm, dat_test, test.gbm = T))
  # save(pred_test, file="../output/pred_test.RData")
  cat("Time for training model = ", tm_test_gbm[1], "s \n")
}
```

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
# cat("Time for constructing training features=", tm_feature_train[1], "s \n")
# cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
# cat("Time for training model=", tm_train[1], "s \n")
# cat("Time for making prediction=", tm_test[1], "s \n")
```
