---
title: "Project 3"
author: "Group 10"
date: "March 19, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

packages.used=c("gbm", "MASS", "OpenImageR", "jpeg", "ggplot2", "reshape2", "randomForest", 
                "e1071", "xgboost")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

## Loading packages
library("EBImage")
library("gbm")
library("MASS")
library("OpenImageR")
library("jpeg")
library("ggplot2")
library("reshape2")
library("randomForest")
library("e1071")
library("xgboost")
```

### Step 0: specify directories.

Set the working directory to the image folder. Specify the training and the testing set.

```{r wkdir, eval=FALSE}
# setwd("../Spring2018-Project3-Group10") 
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
```

Provide directories for raw images. ~~Training set and test set should be in different subfolders. ~~
```{r}
experiment_dir <- "../data/train/" # This will be modified for different data sets.
# img_train_dir <- paste(experiment_dir, "train/", sep="")
# img_test_dir <- paste(experiment_dir, "test/", sep="")
```

### Step 1: set up controls for evaluation experiments.

In this chunk, ,we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set  
+ (number) K, the number of CV folds  
+ (T/F) process features for images  
+ (T/F) run evaluation on an independent test set 
+ (T/F) process features for test set

```{r exp_setup}
run.cv=T # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature=FALSE # process features
run.test=TRUE # run evaluation on an independent test set
run.test.feature = F # process features for test set
```

Which feature extraction method to perform
```{r}
run.sift = F
run.hog = F
run.lbp = F
```

Which model to run
```{r}
run.gbm = F
run.svm = F
run.xgboost = F
run.rf = F
run.logistic = F
```


Using cross-validation or independent test set evaluation, we compare the performance of different classifiers or classifiers with different specifications. 

```{r model_setup}
model_values <- seq(3, 11, 2) # depth for GBM
# model_labels = paste("GBM with depth =", model_values)

svm_values <- seq(0.01, 0.1, by = 0.02) # gamma for svm
# svm_labels = paste("SVM with gamma =", svm_values)

xgboost_values <- seq(0.1, 0.5, by = 0.1) # eta for xgboost
# xgboost_labels = paste("XGBoost with eta =", xgboost_values)

```


### Step 2: construct visual feature and split train and test sets
The baseline model uses scale-invariant feature transform (SIFT) algorithm to extract features, which have 5000 dimensions and are extremely over-weighted. We then have decided to explore a couple of other visual descriptors, including histogram of oriented gradients (HOG) and Local binary patterns (LBP), which produce 55 dimensions and 59 dimensions respectively.  

As we do not have the data with an independent test set, we have create our own testing data by random subsampling. We have used balanced partition to split the data, such that 80% as training set and 20% as test set. In order to obain reproducible results, we have set seed to be 123. 

```{r feature}
# set up feature name
if(run.sift){
   feature_name <- "sift"
}
if(run.hog){
  feature_name <- "hog"
}
if(run.lbp){
  feature_name <- "lbp"
}

# run feature extraction
source("../lib/feature.R")

features <- NULL
if(run.feature){
  features <- feature(experiment_dir, 
                      run.sift = run.sift, run.hog = run.hog, run.lbp = run.lbp,  
                      export = T)
}else{
  if("sift" %in% feature_name){
    load("../output/feature_SIFT.RData")
    features <- sift
  }
  if("hog" %in% feature_name){
    load("../output/feature_HOG.RData")
    features <- hog
  }
  if ("lbp" %in% feature_name){
    features <- read.csv('../output/feature_LBP.csv',header = F)
  }
}

# constructing the feature path name
train_path <- paste0("../output/", feature_name, "_train.csv")
test_path <- paste0("../output/", feature_name, "_test.csv")

# split train and test sets
source("../lib/split_train_test.R")
data <- split_train_test(features)
write.csv(data$train, train_path)
write.csv(data$test, test_path)
#save(data$train, file=paste0("../output/", feature_name, "_train.RData"))
#save(data$test, file=paste0("../output/", feature_name, "_test.RData"))
```

### Step 3: Import train and test sets.

For the example of zip code digits, we code digit 9 as "1" and digit 7 as "0" for binary classification.

```{r train_label}
# read test dataset
dat_test <- read.csv(test_path, header = T)
dat_test <- dat_test[,-1]

# read train dataset
dat_train <- read.csv(train_path, header = T)
dat_train <- dat_train[, 3:length(dat_train[1,])]
label_train <- read.csv(train_path, header = T)
label_train <- label_train[,2]
```

### Step 4: Train a classification model with training images
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
+ Input: a path that points to the training set features.
+ Input: an R object of training sample labels.
+ Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
+ `test.R`
+ Input: a path that points to the test set features.
+ Input: an R object that contains a trained classifier.
+ Output: an R object of class label predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib}
source("../lib/train.R")
source("../lib/test.R")
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters. 

```{r runcv, message=FALSE, warning=FALSE}
source("../lib/cross_validation.R")

if(run.cv){
  if(cv.gbm){
    err_cv <- array(dim=c(length(model_values), 2))
    for(k in 1:length(model_values)){
      cat("k=", k, "\n")
      err_cv[k,] <- cv.function(as.data.frame(dat_train), label_train, model_values[k], K, cv.gbm = T)
    }
  }
  
  if(cv.svm){
    err_cv <- array(dim=c(length(svm_values), 2))
    for(k in 1:length(svm_values)){
      cat("k=", k, "\n")
      err_cv[k,] <- cv.function(as.data.frame(dat_train), label_train, svm_values[k], K, cv.svm = T)
    }
  }
  
  if(cv.xgboost){
    err_cv <- array(dim=c(length(xgboost_values), 2))
    for(k in 1:length(xgboost_values)){
      cat("k=", k, "\n")
      err_cv[k,] <- cv.function(as.data.frame(dat_train), label_train, xgboost_values[k], K, cv.xgboost = T)
    }
  }
  
  save(err_cv, file="../output/err_cv.RData")
}
```

Visualize cross-validation results. 

```{r cv_vis}
if(run.cv){
  if(cv.gbm){
    load("../output/err_cv.RData")
    #pdf("../figs/cv_results.pdf", width=7, height=5)
    plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
         main="Cross Validation Error", type="n", ylim=c(0.25, 0.4))
    points(model_values, err_cv[,1], col="blue", pch=16)
    lines(model_values, err_cv[,1], col="blue")
    arrows(model_values, 
           err_cv[,1]-err_cv[,2], 
           model_values, 
           err_cv[,1]+err_cv[,2], 
           length=0.1, 
           angle=90, 
           code=3)
    #dev.off()
  }
  if(cv.svm){
    load("../output/err_cv.RData")
    #pdf("../figs/cv_results.pdf", width=7, height=5)
    plot(svm_values, err_cv[,1], xlab="SVM gamma values", ylab="CV Error",
         main="Cross Validation Error", type="n", ylim=c(0.30, 0.50))
    points(svm_values, err_cv[,1], col="blue", pch=16)
    lines(svm_values, err_cv[,1], col="blue")
    arrows(svm_values, 
           err_cv[,1]-err_cv[,2], 
           svm_values, 
           err_cv[,1]+err_cv[,2], 
           length=0.1, 
           angle=90, 
           code=3)
    #dev.off()
  }
  if(cv.xgboost){
    load("../output/err_cv.RData")
    #pdf("../figs/cv_results.pdf", width=7, height=5)
    plot(xgboost_values, err_cv[,1], xlab="XGBoost eta values", ylab="CV Error",
         main="Cross Validation Error", type="n", ylim=c(0.30, 0.50))
    points(xgboost_values, err_cv[,1], col="blue", pch=16)
    lines(xgboost_values, err_cv[,1], col="blue")
    arrows(xgboost_values, 
           err_cv[,1]-err_cv[,2], 
           xgboost_values, 
           err_cv[,1]+err_cv[,2], 
           length=0.1, 
           angle=90, 
           code=3)
    #dev.off()
  }
  
}

```


* List cross validation results and choose the "best" parameter value for each model and each feature.
```{r best_model}
source("../lib/cv_result.R")

cv.result <- cv_result()
```

Tune `depth` in GBM
```{r}
cv.result[[1]]
```

Tune `eta` in XGBoost
```{r}
cv.result[[2]]
```


Tune `gamma` in SVM
```{r}
cv.result[[3]]
```

Tune `mtry` and `ntree` in Random Forest for feature hog
```{r}
cv.result[[4]]
```

Tune `mtry` and `ntree` in Random Forest for feature lbp
```{r}
cv.result[[5]]
# min(cv.result[[5]])

```


### Step 5: Train entire training set, compare prediction error and running time

* Train models with the entire training set for each feature using the selected model (model parameter) via cross-validation. Feed the final training model with the completely holdout testing data. 
```{r}
source("../lib/model_compare.R")

run.compare <- F
compare.model(run.compare)

```


